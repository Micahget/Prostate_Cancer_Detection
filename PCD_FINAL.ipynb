{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-12-03T02:24:25.195086Z","iopub.status.busy":"2023-12-03T02:24:25.194757Z","iopub.status.idle":"2023-12-03T02:24:26.787519Z","shell.execute_reply":"2023-12-03T02:24:26.786097Z","shell.execute_reply.started":"2023-12-03T02:24:25.195055Z"},"trusted":true},"outputs":[],"source":["# import libraries\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","%matplotlib inline"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-12-03T02:24:26.792222Z","iopub.status.busy":"2023-12-03T02:24:26.78981Z","iopub.status.idle":"2023-12-03T02:24:26.856864Z","shell.execute_reply":"2023-12-03T02:24:26.855862Z","shell.execute_reply.started":"2023-12-03T02:24:26.792176Z"},"trusted":true},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: './Prostate_Cancer.csv'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./Prostate_Cancer.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(data\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      5\u001b[0m data\u001b[38;5;241m.\u001b[39mhead()\n","File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n","File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n","File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n","File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1706\u001b[0m     f,\n\u001b[0;32m   1707\u001b[0m     mode,\n\u001b[0;32m   1708\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1709\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1710\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1711\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1712\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1713\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1714\u001b[0m )\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n","File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    864\u001b[0m             handle,\n\u001b[0;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    866\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    867\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    869\u001b[0m         )\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n","\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Prostate_Cancer.csv'"]}],"source":["data = pd.read_csv('./Prostate_Cancer.csv')\n","\n","print(data.shape)\n","\n","data.head()"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-12-03T02:24:26.859121Z","iopub.status.busy":"2023-12-03T02:24:26.858108Z","iopub.status.idle":"2023-12-03T02:24:26.884176Z","shell.execute_reply":"2023-12-03T02:24:26.883192Z","shell.execute_reply.started":"2023-12-03T02:24:26.859081Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 100 entries, 0 to 99\n","Data columns (total 10 columns):\n"," #   Column             Non-Null Count  Dtype  \n","---  ------             --------------  -----  \n"," 0   id                 100 non-null    int64  \n"," 1   diagnosis_result   100 non-null    object \n"," 2   radius             100 non-null    int64  \n"," 3   texture            100 non-null    int64  \n"," 4   perimeter          100 non-null    int64  \n"," 5   area               100 non-null    int64  \n"," 6   smoothness         100 non-null    float64\n"," 7   compactness        100 non-null    float64\n"," 8   symmetry           100 non-null    float64\n"," 9   fractal_dimension  100 non-null    float64\n","dtypes: float64(4), int64(5), object(1)\n","memory usage: 7.9+ KB\n"]}],"source":["data.info() "]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-12-03T02:24:26.887636Z","iopub.status.busy":"2023-12-03T02:24:26.887019Z","iopub.status.idle":"2023-12-03T02:24:26.895806Z","shell.execute_reply":"2023-12-03T02:24:26.894848Z","shell.execute_reply.started":"2023-12-03T02:24:26.887598Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>diagnosis_result</th>\n","      <th>radius</th>\n","      <th>texture</th>\n","      <th>perimeter</th>\n","      <th>area</th>\n","      <th>smoothness</th>\n","      <th>compactness</th>\n","      <th>symmetry</th>\n","      <th>fractal_dimension</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>M</td>\n","      <td>23</td>\n","      <td>12</td>\n","      <td>151</td>\n","      <td>954</td>\n","      <td>0.143</td>\n","      <td>0.278</td>\n","      <td>0.242</td>\n","      <td>0.079</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>B</td>\n","      <td>9</td>\n","      <td>13</td>\n","      <td>133</td>\n","      <td>1326</td>\n","      <td>0.143</td>\n","      <td>0.079</td>\n","      <td>0.181</td>\n","      <td>0.057</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>M</td>\n","      <td>21</td>\n","      <td>27</td>\n","      <td>130</td>\n","      <td>1203</td>\n","      <td>0.125</td>\n","      <td>0.160</td>\n","      <td>0.207</td>\n","      <td>0.060</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>M</td>\n","      <td>14</td>\n","      <td>16</td>\n","      <td>78</td>\n","      <td>386</td>\n","      <td>0.070</td>\n","      <td>0.284</td>\n","      <td>0.260</td>\n","      <td>0.097</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>M</td>\n","      <td>9</td>\n","      <td>19</td>\n","      <td>135</td>\n","      <td>1297</td>\n","      <td>0.141</td>\n","      <td>0.133</td>\n","      <td>0.181</td>\n","      <td>0.059</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  diagnosis_result  radius  texture  perimeter  area  smoothness  compactness  \\\n","0                M      23       12        151   954       0.143        0.278   \n","1                B       9       13        133  1326       0.143        0.079   \n","2                M      21       27        130  1203       0.125        0.160   \n","3                M      14       16         78   386       0.070        0.284   \n","4                M       9       19        135  1297       0.141        0.133   \n","\n","   symmetry  fractal_dimension  \n","0     0.242              0.079  \n","1     0.181              0.057  \n","2     0.207              0.060  \n","3     0.260              0.097  \n","4     0.181              0.059  "]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["data.drop(columns=['id'], axis=1, inplace=True) # drop column 'id' from the dataset to avoid overfitting\n","data.head()"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-12-03T02:24:32.922237Z","iopub.status.busy":"2023-12-03T02:24:32.921826Z","iopub.status.idle":"2023-12-03T02:24:33.086245Z","shell.execute_reply":"2023-12-03T02:24:33.085221Z","shell.execute_reply.started":"2023-12-03T02:24:32.922199Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Training data:  (80, 9)\n","Test data:  (20, 9)\n"]}],"source":["# Importing the train_test_split function from sklearn\n","from sklearn.model_selection import train_test_split\n","train, test = train_test_split(data, test_size=0.2, random_state=122)\n","print('Training data: ', train.shape)\n","print('Test data: ', test.shape)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-12-03T02:24:33.087807Z","iopub.status.busy":"2023-12-03T02:24:33.087506Z","iopub.status.idle":"2023-12-03T02:24:33.094889Z","shell.execute_reply":"2023-12-03T02:24:33.093844Z","shell.execute_reply.started":"2023-12-03T02:24:33.08778Z"},"trusted":true},"outputs":[],"source":["# Splitting the data into train and test\n","Xtrain = train.drop(columns=['diagnosis_result'], axis=1) \n","ytrain = train['diagnosis_result'] \n","\n","Xtest = test.drop(columns=['diagnosis_result'], axis=1)\n","ytest = test['diagnosis_result']"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-12-03T02:24:33.096771Z","iopub.status.busy":"2023-12-03T02:24:33.096257Z","iopub.status.idle":"2023-12-03T02:24:36.007506Z","shell.execute_reply":"2023-12-03T02:24:36.006267Z","shell.execute_reply.started":"2023-12-03T02:24:33.096722Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Best Parameters:  {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 10, 'splitter': 'best'}\n","Best Accuracy:  0.8375\n","Test Accuracy:  0.8\n"]}],"source":["# 1st: Decision Tree Classification\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import GridSearchCV\n","\n","tree = DecisionTreeClassifier(random_state=122)\n","\n","param_grid = {\n","    'criterion': ['gini', 'entropy'], \n","    'splitter': ['best', 'random'],\n","    'max_depth': [10, 20, 30],\n","    'min_samples_split': [2, 5, 10],\n","    'min_samples_leaf': [2, 4]\n","}\n","\n","grid_search_tree = GridSearchCV(tree, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n","\n","grid_search_tree.fit(Xtrain, ytrain) # here we are fitting the model to the training data. Which means we are training the model\n","\n","print('Best Parameters: ', grid_search_tree.best_params_)\n","print('Best Accuracy: ', grid_search_tree.best_score_)  # here grid_search_tree.best_score_ is the best accuracy score\n","\n","best_tree = grid_search_tree.best_estimator_   # here best_tree is the best decision tree model\n","\n","test_accuracy_tree = best_tree.score(Xtest, ytest)\n","print('Test Accuracy: ', test_accuracy_tree)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-12-03T02:24:36.505689Z","iopub.status.busy":"2023-12-03T02:24:36.504837Z","iopub.status.idle":"2023-12-03T02:27:03.224215Z","shell.execute_reply":"2023-12-03T02:27:03.222641Z","shell.execute_reply.started":"2023-12-03T02:24:36.505646Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Best Parameters:  {'criterion': 'gini', 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 200}\n","Best Accuracy:  0.8375\n","RandomForestClassifier(max_depth=10, min_samples_leaf=4, n_estimators=200,\n","                       random_state=122)\n","Test Accuracy:  0.9\n"]}],"source":["# 2nd: Random Forest Classification \n","from sklearn.ensemble import RandomForestClassifier\n","\n","forest = RandomForestClassifier(random_state=122)\n","\n","param_grid = {\n","    'n_estimators': [100, 200, 300],\n","    'criterion': ['gini', 'entropy'],\n","    'max_depth': [10, 20, 30],\n","    'min_samples_split': [2, 5, 10],\n","    'min_samples_leaf': [2, 4],\n","    'max_features': ['sqrt', 'log2']\n","}\n","\n","grid_search_forest = GridSearchCV(forest, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n","\n","grid_search_forest.fit(Xtrain, ytrain)\n","\n","print('Best Parameters: ', grid_search_forest.best_params_)\n","print('Best Accuracy: ', grid_search_forest.best_score_)\n","\n","best_forest = grid_search_forest.best_estimator_ # here best_forest is the best random forest model\n","print(best_forest)\n","# test accuracy\n","test_accuracy_forest = best_forest.score(Xtest, ytest) # score method calls the predict method and then compares the predicted values with the actual values\n","print('Test Accuracy: ', test_accuracy_forest)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-12-03T02:27:03.762291Z","iopub.status.busy":"2023-12-03T02:27:03.761177Z","iopub.status.idle":"2023-12-03T02:27:03.778337Z","shell.execute_reply":"2023-12-03T02:27:03.777187Z","shell.execute_reply.started":"2023-12-03T02:27:03.762249Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy of Naive Bayes:  0.7375\n","Test Accuracy:  0.85\n"]}],"source":["# 3rd One: Naive Bayes Classification \n","from sklearn.naive_bayes import MultinomialNB\n","\n","clf = MultinomialNB()\n","mNB = clf.fit(Xtrain,ytrain)\n","ypred = mNB.predict(Xtest)\n","print('Accuracy of Naive Bayes: ', mNB.score(Xtrain, ytrain))\n","test_accuracy = mNB.score(Xtest, ytest)\n","print('Test Accuracy: ', test_accuracy)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-12-03T02:27:04.217451Z","iopub.status.busy":"2023-12-03T02:27:04.216975Z","iopub.status.idle":"2023-12-03T02:27:04.233535Z","shell.execute_reply":"2023-12-03T02:27:04.232425Z","shell.execute_reply.started":"2023-12-03T02:27:04.217384Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Best Parameters:  {'C': 1, 'penalty': 'l1', 'solver': 'saga'}\n","Best Accuracy:  0.85\n","Test Accuracy:  0.95\n"]}],"source":["# 4th Models - Logistic Regression\n","from sklearn.preprocessing import MinMaxScaler\n","\n","# create the MinMaxScaler object\n","scaler = MinMaxScaler()\n","Xtrain_scaled = scaler.fit_transform(Xtrain)\n","Xtest_scaled = scaler.transform(Xtest)\n","\n","from sklearn.linear_model import LogisticRegression\n","\n","# create a logistic regression Classifier\n","logreg = LogisticRegression(random_state=122, max_iter=5000)\n","\n","param_grid = {\n","    'penalty': ['l1', 'l2'],\n","    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n","    'solver': ['liblinear', 'saga']\n","}\n","\n","grid_search_logreg = GridSearchCV(logreg, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n","\n","grid_search_logreg.fit(Xtrain_scaled, ytrain)\n","\n","print('Best Parameters: ', grid_search_logreg.best_params_)\n","print('Best Accuracy: ', grid_search_logreg.best_score_)\n","\n","# get the best model\n","best_logreg = grid_search_logreg.best_estimator_\n","\n","# evaluate the best model on the test set\n","test_accuracy = best_logreg.score(Xtest_scaled, ytest)\n","print('Test Accuracy: ', test_accuracy)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # 5th model: Support Vector Machine\n","# from sklearn.svm import SVC\n","# from sklearn.model_selection import GridSearchCV\n","\n","# svc_model = SVC(random_state=122)\n","\n","# # Parameter grid for SVM\n","# param_grid = {\n","#     'C': [0.1, 1, 10, 100],\n","#     'gamma': [1, 0.1, 0.01, 0.001],\n","#     'kernel': ['rbf', 'poly', 'sigmoid']\n","# }\n","# # \n","# grid_search_svc = GridSearchCV(svc_model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n","\n","# grid_search_svc.fit(Xtrain, ytrain)\n","\n","# # Access and print the results during grid search\n","# results = grid_search_svc.cv_results_\n","# for mean_score, params in zip(results['mean_test_score'], results['params']):\n","#     print(f'Mean accuracy for {params}: {mean_score:.3f}')\n","\n","# print('Best Parameters: ', grid_search_svc.best_params_)\n","# print('Best Accuracy: ', grid_search_svc.best_score_)\n","\n","# best_svc = grid_search_svc.best_estimator_\n","\n","# test_accuracy = best_svc.score(Xtest, ytest)\n","# print('Test Accuracy: ', test_accuracy)\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Hybrid of DT and RF Test Accuracy:  0.85\n","Hybrid of DT and RF Train Accuracy:  0.9125\n"]}],"source":["# H1 : Hybrid of DT and RF\n","from sklearn.ensemble import VotingClassifier\n","\n","voting_clf_DT_RF = VotingClassifier(estimators=[\n","    ('decision_tree', best_tree),   \n","    ('random_forest', best_forest) \n","], voting='hard') \n","\n","voting_clf_DT_RF.fit(Xtrain, ytrain)\n","\n","accuracy_DT_RF_test = voting_clf_DT_RF.score(Xtest, ytest)\n","accuracy_DT_RF_train = voting_clf_DT_RF.score(Xtrain, ytrain)\n","print('Hybrid of DT and RF Test Accuracy: ', accuracy_DT_RF_test)\n","print('Hybrid of DT and RF Train Accuracy: ', accuracy_DT_RF_train)\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Hybrid of LR and RF Test Accuracy:  0.9\n","Hybrid of LR and RF Train Accuracy:  0.8625\n"]}],"source":["# H2: Hybrid of LR and RF\n","voting_clf_LR_RF = VotingClassifier(estimators=[\n","    ('Logistic_reg', best_logreg),\n","    ('random_forest', best_forest) \n","], voting='hard')  \n","\n","voting_clf_LR_RF.fit(Xtrain, ytrain)\n","\n","accuracy_LR_RF_test = voting_clf_LR_RF.score(Xtest, ytest)\n","accuracy_LR_RF_train = voting_clf_LR_RF.score(Xtrain, ytrain)\n","print('Hybrid of LR and RF Test Accuracy: ', accuracy_LR_RF_test)\n","print('Hybrid of LR and RF Train Accuracy: ', accuracy_LR_RF_train)"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Hybrid of LR and DT Test Accuracy:  0.85\n","Hybrid of LR and DT Train Accuracy:  0.8625\n"]}],"source":["# H3: Hybrid of LR and DT\n","voting_clf_LR_DT = VotingClassifier(estimators=[\n","    ('Logistic_reg', best_logreg),\n","    ('decision_tree', best_tree)\n","], voting='hard')  \n","\n","voting_clf_LR_DT.fit(Xtrain, ytrain)\n","\n","accuracy_LR_DT_test = voting_clf_LR_DT.score(Xtest, ytest)\n","accuracy_LR_DT_train = voting_clf_LR_DT.score(Xtrain, ytrain)\n","print('Hybrid of LR and DT Test Accuracy: ', accuracy_LR_DT_test)\n","print('Hybrid of LR and DT Train Accuracy: ', accuracy_LR_DT_train)"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Best Parameters:  {'weights': [1, 2]}\n","Best Accuracy:  0.8375\n","Hybrid of LR and DT Test Accuracy:  0.8\n"]}],"source":["# Hyperparameter Tuning for the Voting Classifier\n","from sklearn.ensemble import VotingClassifier\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","\n","# Define the estimators\n","estimators = [\n","    ('Logistic_reg', best_logreg),\n","    ('decision_tree', best_tree)\n","]\n","\n","# Create the VotingClassifier\n","voting_clf_LR_DTH = VotingClassifier(estimators=estimators, voting='hard')\n","\n","# Define the parameter grid for Logistic Regression\n","param_grid_logreg = {\n","    'C': [0.1, 1, 10],\n","    'solver': ['liblinear', 'lbfgs', 'sag', 'saga']\n","}\n","\n","# Define the parameter grid for Decision Tree\n","param_grid_tree = {\n","    'max_depth': [None, 10, 20, 30],\n","    'min_samples_split': [2, 5, 10],\n","    'min_samples_leaf': [1, 2, 4]\n","}\n","\n","# Define the parameter grid for the VotingClassifier\n","param_grid_voting = {\n","    'weights': [[1,1], [1,2], [2,1], [2,2]]  # Different combinations of weights for LR and DT\n","}\n","\n","# Create the GridSearchCV object\n","grid_search = GridSearchCV(voting_clf_LR_DTH, param_grid=param_grid_voting, cv=5, scoring='accuracy')\n","\n","# Fit the GridSearchCV to the data\n","grid_search.fit(Xtrain, ytrain)\n","\n","# Print the best parameters and best accuracy\n","print('Best Parameters: ', grid_search.best_params_)\n","print('Best Accuracy: ', grid_search.best_score_)\n","\n","# Access the best estimator\n","best_voting_clf_LR_DT = grid_search.best_estimator_\n","\n","# Calculate accuracy on test set\n","accuracy_LR_DT_test = best_voting_clf_LR_DT.score(Xtest, ytest)\n","print('Hybrid of LR and DT Test Accuracy: ', accuracy_LR_DT_test)\n"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Hybrid of RF and NB Test Accuracy:  0.85\n","Hybrid of RF and NB Train Accuracy:  0.7875\n"]}],"source":["# H4: Hybrid of RF and NB\n","voting_clf_RF_NB = VotingClassifier(estimators=[\n","    ('random_forest', best_forest),\n","    ('Naive_Bayes', mNB)\n","], voting='hard')\n","\n","voting_clf_RF_NB.fit(Xtrain, ytrain)\n","\n","accuracy_RF_NB_test = voting_clf_RF_NB.score(Xtest, ytest)\n","accuracy_RF_NB_train = voting_clf_RF_NB.score(Xtrain, ytrain)\n","\n","print('Hybrid of RF and NB Test Accuracy: ', accuracy_RF_NB_test)\n","print('Hybrid of RF and NB Train Accuracy: ', accuracy_RF_NB_train)"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Hybrid of LR and DT and RF Test Accuracy:  0.9\n","Hybrid of LR and DT and RF Train Accuracy:  0.9125\n"]}],"source":["# H5: Hybrid of LR and DT and RF\n","voting_clf_LR_DT_RF = VotingClassifier(estimators=[\n","    ('Logistic_reg', best_logreg),\n","    ('decision_tree', best_tree),\n","    ('random_forest', best_forest)\n","], voting='hard')\n","\n","voting_clf_LR_DT_RF.fit(Xtrain, ytrain) \n","\n","accuracy_LR_DT_RF_test = voting_clf_LR_DT_RF.score(Xtest, ytest) \n","accuracy_LR_DT_RF_train = voting_clf_LR_DT_RF.score(Xtrain, ytrain)\n","\n","print('Hybrid of LR and DT and RF Test Accuracy: ', accuracy_LR_DT_RF_test)\n","print('Hybrid of LR and DT and RF Train Accuracy: ', accuracy_LR_DT_RF_train)\n"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Best Parameters:  {'weights': [1, 1, 2]}\n","Best Accuracy:  0.8375\n","Hybrid of LR, DT, and RF Test Accuracy:  0.9\n"]}],"source":["# Hyperparameter Tuning for the Voting Classifier\n","from sklearn.ensemble import VotingClassifier\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","\n","# Define the estimators\n","estimators = [\n","    ('Logistic_reg', best_logreg),\n","    ('decision_tree', best_tree),\n","    ('random_forest', best_forest)\n","]\n","\n","# Create the VotingClassifier\n","voting_clf_LR_DT_RFH = VotingClassifier(estimators=estimators, voting='hard')\n","\n","# Define the parameter grid for Logistic Regression\n","param_grid_logreg = {\n","    'C': [0.1, 1, 10],\n","    'solver': ['liblinear', 'lbfgs', 'sag', 'saga']\n","}\n","\n","# Define the parameter grid for Decision Tree\n","param_grid_tree = {\n","    'max_depth': [None, 10, 20, 30],\n","    'min_samples_split': [2, 5, 10],\n","    'min_samples_leaf': [1, 2, 4]\n","}\n","\n","# Define the parameter grid for Random Forest\n","param_grid_forest = {\n","    'n_estimators': [50, 100, 200],\n","    'max_depth': [None, 10, 20],\n","    'min_samples_split': [2, 5, 10],\n","    'min_samples_leaf': [1, 2, 4]\n","}\n","\n","# Define the parameter grid for the VotingClassifier\n","param_grid_voting = {\n","    'weights': [[1,1,1], [1,2,1], [1,1,2], [2,1,1]]  # Different combinations of weights for LR, DT, and RF\n","}\n","\n","# Create the GridSearchCV object\n","grid_search = GridSearchCV(voting_clf_LR_DT_RFH, param_grid=param_grid_voting, cv=5, scoring='accuracy')\n","\n","# Fit the GridSearchCV to the data\n","grid_search.fit(Xtrain, ytrain)\n","\n","# Print the best parameters and best accuracy\n","print('Best Parameters: ', grid_search.best_params_)\n","print('Best Accuracy: ', grid_search.best_score_)\n","\n","# Access the best estimator\n","best_voting_clf_LR_DT_RF = grid_search.best_estimator_\n","\n","# Calculate accuracy on test set\n","accuracy_LR_DT_RF_test = best_voting_clf_LR_DT_RF.score(Xtest, ytest)\n","print('Hybrid of LR, DT, and RF Test Accuracy: ', accuracy_LR_DT_RF_test)\n"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Hybrid of All  0.9\n","Hybrid of all train  0.8625\n"]}],"source":["# H : Hybrid of all 4\n","voting_clf_ALL = VotingClassifier(estimators=[\n","    ('Logistic_reg', best_logreg),\n","    ('decision_tree', best_tree),\n","    ('Naive Bayes', mNB),\n","    ('Random Forest', best_forest)\n","], voting='hard')  \n","\n","voting_clf_ALL.fit(Xtrain, ytrain)\n","\n","accuracy_ALL_test = voting_clf_ALL.score(Xtest, ytest)\n","accuracy_ALL_train = voting_clf_ALL.score(Xtrain, ytrain)\n","print('Hybrid of All ', accuracy_ALL_test)\n","print('Hybrid of all train ', accuracy_ALL_train)\n"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Best Parameters:  {'weights': [1, 1, 1, 1]}\n","Best Accuracy:  0.8375\n","Hybrid of All Test Accuracy:  0.9\n"]}],"source":["from sklearn.ensemble import VotingClassifier\n","from sklearn.model_selection import GridSearchCV\n","# from sklearn.linear_model import LogisticRegression\n","# from sklearn.tree import DecisionTreeClassifier\n","# from sklearn.naive_bayes import MultinomialNB\n","# from sklearn.ensemble import RandomForestClassifier\n","\n","# Define the estimators\n","estimators = [\n","    ('Logistic_reg',  best_forest),\n","    ('decision_tree', best_forest),\n","    ('Naive Bayes', best_forest),\n","    ('Random Forest', best_forest)\n","]\n","\n","# Create the VotingClassifier\n","voting_clf_ALLH = VotingClassifier(estimators=estimators, voting='hard')\n","\n","# Define the parameter grid for Logistic Regression\n","param_grid_logreg = {\n","    'C': [0.1, 1, 10],\n","    'solver': ['liblinear', 'lbfgs', 'sag', 'saga']\n","}\n","\n","# Define the parameter grid for Decision Tree\n","param_grid_tree = {\n","    'max_depth': [None, 10, 20, 30],\n","    'min_samples_split': [2, 5, 10],\n","    'min_samples_leaf': [1, 2, 4]\n","}\n","\n","# Define the parameter grid for Random Forest\n","param_grid_forest = {\n","    'n_estimators': [50, 100, 200],\n","    'max_depth': [None, 10, 20],\n","    'min_samples_split': [2, 5, 10],\n","    'min_samples_leaf': [1, 2, 4]\n","}\n","\n","# Define the parameter grid for the VotingClassifier\n","param_grid_voting = {\n","    'weights': [[1,1,1,1], [1,2,1,1], [1,1,2,1], [1,1,1,2], [2,1,1,1]]  # Different combinations of weights for LR, DT, NB, and RF\n","}\n","\n","# Create the GridSearchCV object\n","grid_search = GridSearchCV(voting_clf_ALLH, param_grid=param_grid_voting, cv=5, scoring='accuracy')\n","\n","# Fit the GridSearchCV to the data\n","grid_search.fit(Xtrain, ytrain)\n","\n","# Print the best parameters and best accuracy\n","print('Best Parameters: ', grid_search.best_params_)\n","print('Best Accuracy: ', grid_search.best_score_)\n","\n","# Access the best estimator\n","best_voting_clf_ALL = grid_search.best_estimator_\n","\n","# Calculate accuracy on test set\n","accuracy_ALL_test = best_voting_clf_ALL.score(Xtest, ytest)\n","print('Hybrid of All Test Accuracy: ', accuracy_ALL_test)\n"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2023-12-03T02:27:51.950677Z","iopub.status.busy":"2023-12-03T02:27:51.950285Z","iopub.status.idle":"2023-12-03T02:27:51.957734Z","shell.execute_reply":"2023-12-03T02:27:51.956678Z","shell.execute_reply.started":"2023-12-03T02:27:51.950642Z"},"trusted":true},"outputs":[],"source":["# comparision of the models\n","from sklearn.metrics import confusion_matrix, classification_report, precision_recall_fscore_support, roc_auc_score\n","\n","def performance_evaluation(X,y,clf,classifier_name=''):\n","    ypred = clf.predict(X)\n","    accuracy = clf.score(X,y)\n","    precision, recall, fscore, support = precision_recall_fscore_support(y, ypred, average='weighted', zero_division=0)\n","    metricName = ['Accuracy','Precision','Recall','F1_Score']\n","    metricValue = [accuracy,precision,recall,fscore]\n","    res = pd.DataFrame(metricValue, index=metricName, columns=[classifier_name])\n","    return res"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Decision Tree</th>\n","      <th>Random Forest</th>\n","      <th>MultinomialNB</th>\n","      <th>Logistic Regression</th>\n","      <th>DT_RF</th>\n","      <th>LR_RF</th>\n","      <th>LR_DT</th>\n","      <th>RF_NB</th>\n","      <th>LR_DT_RF</th>\n","      <th>ALL</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Accuracy</th>\n","      <td>0.8</td>\n","      <td>0.9</td>\n","      <td>0.850</td>\n","      <td>0.950000</td>\n","      <td>0.850000</td>\n","      <td>0.9</td>\n","      <td>0.850000</td>\n","      <td>0.850</td>\n","      <td>0.9</td>\n","      <td>0.9</td>\n","    </tr>\n","    <tr>\n","      <th>Precision</th>\n","      <td>0.8</td>\n","      <td>0.9</td>\n","      <td>0.900</td>\n","      <td>0.953333</td>\n","      <td>0.860440</td>\n","      <td>0.9</td>\n","      <td>0.860440</td>\n","      <td>0.900</td>\n","      <td>0.9</td>\n","      <td>0.9</td>\n","    </tr>\n","    <tr>\n","      <th>Recall</th>\n","      <td>0.8</td>\n","      <td>0.9</td>\n","      <td>0.850</td>\n","      <td>0.950000</td>\n","      <td>0.850000</td>\n","      <td>0.9</td>\n","      <td>0.850000</td>\n","      <td>0.850</td>\n","      <td>0.9</td>\n","      <td>0.9</td>\n","    </tr>\n","    <tr>\n","      <th>F1_Score</th>\n","      <td>0.8</td>\n","      <td>0.9</td>\n","      <td>0.856</td>\n","      <td>0.948589</td>\n","      <td>0.852991</td>\n","      <td>0.9</td>\n","      <td>0.852991</td>\n","      <td>0.856</td>\n","      <td>0.9</td>\n","      <td>0.9</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["           Decision Tree  Random Forest  MultinomialNB  Logistic Regression  \\\n","Accuracy             0.8            0.9          0.850             0.950000   \n","Precision            0.8            0.9          0.900             0.953333   \n","Recall               0.8            0.9          0.850             0.950000   \n","F1_Score             0.8            0.9          0.856             0.948589   \n","\n","              DT_RF  LR_RF     LR_DT  RF_NB  LR_DT_RF  ALL  \n","Accuracy   0.850000    0.9  0.850000  0.850       0.9  0.9  \n","Precision  0.860440    0.9  0.860440  0.900       0.9  0.9  \n","Recall     0.850000    0.9  0.850000  0.850       0.9  0.9  \n","F1_Score   0.852991    0.9  0.852991  0.856       0.9  0.9  "]},"metadata":{},"output_type":"display_data"}],"source":["# generate the performance summary for various models trained\n","tree_summary = performance_evaluation(Xtest,ytest,best_tree,'Decision Tree')\n","forest_summary = performance_evaluation(Xtest,ytest,best_forest,'Random Forest')\n","multinomialNB_summary = performance_evaluation(Xtest,ytest,mNB,'MultinomialNB')\n","logistic_summary = performance_evaluation(Xtest_scaled,ytest,best_logreg,'Logistic Regression')\n","hybrid_sum_DT_RF = performance_evaluation(Xtest,ytest,voting_clf_DT_RF,'DT_RF')\n","hybrid_sum_LR_RF = performance_evaluation(Xtest,ytest,voting_clf_LR_RF,'LR_RF')\n","hybrid_sum_LR_DT = performance_evaluation(Xtest,ytest,voting_clf_LR_DT,'LR_DT')\n","hybrid_sum_RF_NB = performance_evaluation(Xtest,ytest,voting_clf_RF_NB,'RF_NB')\n","hybrid_sum_LR_DT_RF = performance_evaluation(Xtest,ytest,voting_clf_LR_DT_RF,'LR_DT_RF')\n","hybrid_sum_ALL = performance_evaluation(Xtest,ytest,voting_clf_ALL,'ALL')\n","\n","# combine the summary of each model into a dataframe \n","comparison_df = pd.concat([tree_summary,forest_summary,multinomialNB_summary,logistic_summary,hybrid_sum_DT_RF,hybrid_sum_LR_RF,hybrid_sum_LR_DT,hybrid_sum_RF_NB,hybrid_sum_LR_DT_RF,hybrid_sum_ALL], axis=1)\n","\n","# disply the summary dafaframe\n","display(comparison_df)"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Best Parameters (Decision Tree):  {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 10, 'splitter': 'best'}\n","Best Accuracy (Decision Tree):  0.8375\n","Best Parameters (Random Forest):  {'criterion': 'gini', 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 200}\n","Best Accuracy (Random Forest):  0.8375\n","Hybrid Accuracy: 0.85\n"]}],"source":["from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import accuracy_score\n","\n","# Decision Tree model\n","tree = DecisionTreeClassifier(random_state=122)\n","\n","# Define the hyperparameter grid\n","param_grid_tree = {\n","    'criterion': ['gini', 'entropy'],\n","    'splitter': ['best', 'random'],\n","    'max_depth': [10, 20, 30],\n","    'min_samples_split': [2, 5, 10],\n","    'min_samples_leaf': [2, 4]\n","}\n","\n","# GridSearchCV for Decision Tree\n","grid_search_tree = GridSearchCV(tree, param_grid_tree, cv=5, scoring='accuracy', n_jobs=-1)\n","grid_search_tree.fit(Xtrain, ytrain)\n","\n","# Print the best parameters and corresponding accuracy\n","print('Best Parameters (Decision Tree): ', grid_search_tree.best_params_)\n","print('Best Accuracy (Decision Tree): ', grid_search_tree.best_score_)\n","\n","# Get the best Decision Tree model\n","best_tree = grid_search_tree.best_estimator_\n","\n","# Random Forest model\n","forest = RandomForestClassifier(random_state=122)\n","\n","# Define the hyperparameter grid\n","param_grid_forest = {\n","    'n_estimators': [100, 200, 300],\n","    'criterion': ['gini', 'entropy'],\n","    'max_depth': [10, 20, 30],\n","    'min_samples_split': [2, 5, 10],\n","    'min_samples_leaf': [2, 4],\n","    'max_features': ['sqrt', 'log2']\n","}\n","\n","# GridSearchCV for Random Forest\n","grid_search_forest = GridSearchCV(forest, param_grid_forest, cv=5, scoring='accuracy', n_jobs=-1)\n","grid_search_forest.fit(Xtrain, ytrain)\n","\n","# Print the best parameters and corresponding accuracy\n","print('Best Parameters (Random Forest): ', grid_search_forest.best_params_)\n","print('Best Accuracy (Random Forest): ', grid_search_forest.best_score_)\n","\n","# Get the best Random Forest model\n","best_forest = grid_search_forest.best_estimator_\n","\n","# Combine predictions using a majority voting scheme\n","hybrid_predictions = []\n","for i in range(len(Xtest)):\n","    tree_prediction = best_tree.predict(Xtest)[i]\n","    forest_prediction = best_forest.predict(Xtest)[i]\n","\n","    predictions = [tree_prediction, forest_prediction]\n","    most_common_prediction = max(set(predictions), key=predictions.count)\n","    hybrid_predictions.append(most_common_prediction)\n","\n","# Evaluate the accuracy of the hybrid model on the test set\n","hybrid_accuracy = accuracy_score(ytest, hybrid_predictions)\n","print(\"Hybrid Accuracy:\", hybrid_accuracy)\n"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Voting Classifier Accuracy (Decision Tree and Naive Bayes):  0.8\n"]}],"source":["# Hybrid of Decision Tree and Naive Bayes\n","from sklearn.ensemble import VotingClassifier\n","\n","# Create a Voting Classifier\n","voting_clf = VotingClassifier(estimators=[\n","    # ('logistic_regression', best_logreg),   # best_tree is the best Decision Tree model\n","    # ('random_forest', best_forest),\n","    ('decision_tree', best_tree),\n","    ('naive_bayes', mNB)\n","], voting='hard')  # 'hard' voting means majority voting\n","\n","# Fit the Voting Classifier to the data\n","voting_clf.fit(Xtrain, ytrain)\n","\n","# Evaluate the accuracy of the Voting Classifier on the test set\n","accuracy_tree_nb = voting_clf.score(Xtest, ytest)\n","print('Voting Classifier Accuracy (Decision Tree and Naive Bayes): ', accuracy_tree_nb)"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Voting Classifier Accuracy (Logistic Regression and Random Forest):  0.9\n"]}],"source":["# Hybrid of Logistic Regression and Random Forest\n","from sklearn.ensemble import VotingClassifier\n","\n","# Create a Voting Classifier\n","voting_clf = VotingClassifier(estimators=[\n","    ('logistic_regression', best_logreg),   # best_tree is the best Decision Tree model\n","    ('random_forest', best_forest),\n","    # ('decision_tree', best_tree),\n","    # ('naive_bayes', mNB)\n","], voting='hard')  # 'hard' voting means majority voting\n","\n","# Fit the Voting Classifier to the data\n","voting_clf.fit(Xtrain, ytrain)\n","\n","# Evaluate the accuracy of the Voting Classifier on the test set\n","accuracy_logreg_forest = voting_clf.score(Xtest, ytest)\n","print('Voting Classifier Accuracy (Logistic Regression and Random Forest): ', accuracy_logreg_forest)"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Voting Classifier Accuracy (Logistic Regression and Decision Tree):  0.85\n"]}],"source":["# Hybrid of Logistic Regression and Decision\n","from sklearn.ensemble import VotingClassifier\n","\n","# Create a Voting Classifier\n","voting_clf = VotingClassifier(estimators=[\n","    ('logistic_regression', best_logreg),   # best_tree is the best Decision Tree model\n","    # ('random_forest', best_forest),\n","    ('decision_tree', best_tree),\n","    # ('naive_bayes', mNB)\n","], voting='hard')  # 'hard' voting means majority voting\n","\n","# Fit the Voting Classifier to the data\n","voting_clf.fit(Xtrain, ytrain)\n","\n","# Evaluate the accuracy of the Voting Classifier on the test set\n","accuracy_logreg_forest = voting_clf.score(Xtest, ytest)\n","print('Voting Classifier Accuracy (Logistic Regression and Decision Tree): ', accuracy_logreg_forest)"]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Voting Classifier Accuracy (Logistic Regression and Naive Bayes):  0.85\n"]}],"source":["# Hybrid of Logistic Regression and Naive Bayes\n","from sklearn.ensemble import VotingClassifier\n","\n","# Create a Voting Classifier\n","voting_clf = VotingClassifier(estimators=[\n","    ('logistic_regression', best_logreg),   # best_tree is the best Decision Tree model\n","    # ('random_forest', best_forest),\n","    # ('decision_tree', best_tree),\n","    ('naive_bayes', mNB)\n","], voting='hard')  # 'hard' voting means majority voting\n","\n","# Fit the Voting Classifier to the data\n","voting_clf.fit(Xtrain, ytrain)\n","\n","# Evaluate the accuracy of the Voting Classifier on the test set\n","accuracy_logreg_mNB = voting_clf.score(Xtest, ytest)\n","print('Voting Classifier Accuracy (Logistic Regression and Naive Bayes): ', accuracy_logreg_mNB)"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Voting Classifier Accuracy (Logistic Regression, Random Forest and Decision Tree):  0.9\n"]}],"source":["# Hybrid of Random Forest and Decision Tree and Logistic Regression\n","from sklearn.ensemble import VotingClassifier\n","\n","# Create a Voting Classifier\n","voting_clf = VotingClassifier(estimators=[\n","    ('logistic_regression', best_logreg),   # best_tree is the best Decision Tree model\n","    ('random_forest', best_forest),\n","    ('decision_tree', best_tree),\n","    # ('naive_bayes', mNB)\n","], voting='hard')  # 'hard' voting means majority voting\n","\n","# Fit the Voting Classifier to the data\n","voting_clf.fit(Xtrain, ytrain)\n","\n","# Evaluate the accuracy of the Voting Classifier on the test set\n","accuracy_logreg_forest_tree = voting_clf.score(Xtest, ytest)\n","print('Voting Classifier Accuracy (Logistic Regression, Random Forest and Decision Tree): ', accuracy_logreg_forest_tree)"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Voting Classifier Accuracy (Logistic Regression, Random Forest, Decision Tree and Naive Bayes):  0.9\n"]}],"source":["# Hybrid of logistic regression, random forest, decision tree and naive bayes\n","from sklearn.ensemble import VotingClassifier\n","\n","# Create a Voting Classifier\n","voting_clf = VotingClassifier(estimators=[\n","    ('logistic_regression', best_logreg),   # best_tree is the best Decision Tree model\n","    ('random_forest', best_forest),\n","    ('decision_tree', best_tree),\n","    ('naive_bayes', mNB)\n","], voting='hard')  # 'hard' voting means majority voting\n","\n","# Fit the Voting Classifier to the data\n","voting_clf.fit(Xtrain, ytrain)\n","\n","# Evaluate the accuracy of the Voting Classifier on the test set\n","accuracy_logreg_forest_tree_mNB = voting_clf.score(Xtest, ytest)\n","print('Voting Classifier Accuracy (Logistic Regression, Random Forest, Decision Tree and Naive Bayes): ', accuracy_logreg_forest_tree_mNB)"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[],"source":["# comparision of the models\n","# define a helper function to perform model evaluation based on key metrics\n","from sklearn.metrics import confusion_matrix, classification_report, precision_recall_fscore_support, roc_auc_score\n","\n","def performance_evaluation(X,y,clf,classifier_name=''):\n","    ypred = clf.predict(X)\n","    accuracy = clf.score(X,y)\n","    precision, recall, fscore, support = precision_recall_fscore_support(y, ypred, average='weighted', zero_division=0)\n","    metricName = ['Accuracy','Precision','Recall','F1_Score']\n","    metricValue = [accuracy,precision,recall,fscore]\n","    res = pd.DataFrame(metricValue, index=metricName, columns=[classifier_name])\n","    return res"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":66762,"sourceId":131607,"sourceType":"datasetVersion"}],"dockerImageVersionId":30587,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":4}
